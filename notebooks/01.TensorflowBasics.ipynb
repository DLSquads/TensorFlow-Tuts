{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Basics\n",
    "TensorFlow is a Deep Learning library which works on the concept of computational graphs! Every operation done on a Tensor (n-d array) is represented using a node of the graph!\n",
    "\n",
    "Let's learn it by an example. Say you want to perform following operation on a matrix `X` to get matrix `Y`:\n",
    "$$Y = X\\times2 + 3$$\n",
    "\n",
    "Here,\n",
    "* $\\times$: represents element-wise multiplication and\n",
    "* $+$: represents element-wise addition\n",
    "\n",
    "TensorFlow behind the scenes represent this operation inform of a computational graph as this:\n",
    "\n",
    "![Computational Graph](../images/computational-graph.png)\n",
    "\n",
    "Let's code this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.Variable([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) # make a variable\n",
    "const2 = tf.constant(2.)  # make a constant\n",
    "const3 = tf.constant(3.)  # make other constat\n",
    "\n",
    "operation1 = X * const2 # elementwise multiplication operation\n",
    "y = operation1 + const3 # result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3, 3) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the...! Where's our result? TensorFlow works on what we call a static computational graph! This means that whenever we create a variable `X = tf.Variable(...)` or create a constant `const_x = tf.constant(...)` or any other operation, we are only building nodes of a graph and are **not executing them**.\n",
    "\n",
    "To execute a complete graph we need to make a `tf.Session()`. Consider this session as an environment which makes the computational graph go active!\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  7.,  9.],\n",
       "       [11., 13., 15.],\n",
       "       [17., 19., 21.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session() # make a session object\n",
    "\n",
    "sess.run(tf.global_variables_initializer()) # initialize variables\n",
    "sess.run(y) # run all nodes that meet up till y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why are we running `tf.global_variables_initializer()`? Remember when we make a variable using `tf.Variable(...)`, we are telling the computational graph that session should initialize variable with data, i.e., we aren't initializing variable until and unless we tell session to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all fine and hunky-dory but there are situations where we don't like to write these extra session lines. And what's with all those constant declartions? Can't we just use values directly?\n",
    "\n",
    "Answer to all this is yes we can. To improvise such scenario's TensorFlow provides us with `tf.enable_eager_execution()`. Eager execution allows us to run the nodes of graph then and there in a more *pythonic* manner!\n",
    "\n",
    "> Now remember that eager execution should be ran on startup! So for this sake let's restart our kernel and start again from next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=13, shape=(3, 3), dtype=float32, numpy=\n",
       " array([[ 5.,  7.,  9.],\n",
       "        [11., 13., 15.],\n",
       "        [17., 19., 21.]], dtype=float32)>, array([[ 5.,  7.,  9.],\n",
       "        [11., 13., 15.],\n",
       "        [17., 19., 21.]], dtype=float32))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# contrib call will change to tf.Variable soon\n",
    "X = tf.contrib.eager.Variable([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) # make a variable\n",
    "y = X * 2. + 3.\n",
    "\n",
    "y, y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through some advantages or disadvantages of the two computational graphs: static and dynamic\n",
    "\n",
    "\n",
    "|                 |Static Computational Graph|Dynamic Computational Graph|\n",
    "|:----------------|:-------------------------|:--------------------------|\n",
    "|**Advantages**   |1. We only have one computational graph that is called over and over on each call|1. Easy to debug|\n",
    "|                 |2. Each backward pass doesn't require creation of new graph|2. Flexible in controlling flow of program|\n",
    "|                 |3. static graphs can optimize the graph up front while dynamic graph can not do it.|3. Easy to understand than static graphs. Especially for new comers who are familiar with python.|\n",
    "|**Disadvantages**|1. Sometimes become difficult to debug (we need to run complete graph to find out a logical error at first step)|1. Each forward pass defines a new computational graph. This slows it down|\n",
    "|                 |2. Flow of control requires extra care. Example: creating nodes inside a loop can lead to large number of nodes lying in the memory as old nodes aren't deleted!|2. Each backward pass creates new computational graph as well. This slows down training!|\n",
    "|                 |3. Not so pythonic and difficult to understand for new comers|3. Static graphs can optimize up from while static graphs can not do it!|\n",
    "\n",
    "Enough with the theory let's implement a linear regression model here. First we will be looking at dynamic computational graph example and then we will be looking at static computational graph example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Dynamic Computational Graph\n",
    "\n",
    "First let's create a synthetic dataset to work on! We will be generating a dataset that defines this line:\n",
    "\n",
    "$$y = 2x + 3$$\n",
    "\n",
    "but we will add some noise to the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(2000, 1)\n",
    "y = X * 2 + 3 + np.random.randn(*X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 1), (2000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define prediction function and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, weight, bias):\n",
    "    return inputs * weight + bias\n",
    "\n",
    "def loss(inputs, weight, bias, outputs):\n",
    "    error = predict(inputs, weight, bias) - outputs\n",
    "    return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will define our weight and bias - coefficient of X (slope of line) and bias (y intercept)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-0.24387714], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.contrib.eager.Variable(tf.random_normal([1]))\n",
    "b = tf.contrib.eager.Variable(tf.zeros([1]))\n",
    "\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!\n",
    "\n",
    "You will notice that the below cell is ran multiple times! We run it again and again until we get to satisfactory loss value.\n",
    "\n",
    "But what's with `GradientTape`? According to TensorFlow's official documentation we get a jargon like this:\n",
    "\n",
    "> `tf.GradientTape` is an opt-in feature to provide maximal performance when not tracing. Since different operations can occur during each call, all forward-pass operations get recorded to a \"tape\". To compute the gradient, play the tape backwards and then discard. A particular `tf.GradientTape` can only compute one gradient; subsequent calls throw a runtime error.\n",
    "\n",
    "Tl;dr - Use `tf.GradientTape` it to boost performance and get your gradients. Just call it once for every gradient call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200, loss: 1.2570668458938599\n",
      "Epoch: 20/200, loss: 1.2370402812957764\n",
      "Epoch: 40/200, loss: 1.2185415029525757\n",
      "Epoch: 60/200, loss: 1.2014540433883667\n",
      "Epoch: 80/200, loss: 1.1856707334518433\n",
      "Epoch: 100/200, loss: 1.1710915565490723\n",
      "Epoch: 120/200, loss: 1.157624363899231\n",
      "Epoch: 140/200, loss: 1.1451847553253174\n",
      "Epoch: 160/200, loss: 1.1336944103240967\n",
      "Epoch: 180/200, loss: 1.123080849647522\n",
      "Final loss: 1.114\n",
      "W = [1.799801], B = [2.7217016]\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 0.001\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        l = loss(X, w, b, y)\n",
    "    dw, db = tape.gradient(l, [w, b])\n",
    "    w.assign_sub(dw * lr)\n",
    "    b.assign_sub(db * lr)\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch: {}/{}, loss: {}'.format(epoch, epochs, loss(X, w, b, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(X, w, b, y)))\n",
    "print(\"W = {}, B = {}\".format(w.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Static Computational Graph\n",
    "\n",
    "We are currently executing graphs in dynamic mode. Let's change to static mode! To do we will need to restart kernel. After restarting kernel let's start building our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(2000, 1)\n",
    "y = X * 2 + 3 + np.random.randn(*X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 1), (2000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, weight, bias):\n",
    "    return inputs * weight + bias\n",
    "\n",
    "def loss(inputs, weight, bias, outputs):\n",
    "    error = predict(inputs, weight, bias) - outputs\n",
    "    return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational graphs require inputs nodes to interact with incoming data. This data is fed into the graph using special nodes called placeholders. These variables takes into account input shape and the data type of the input.\n",
    "\n",
    "Our graph require inputs:\n",
    "* X\n",
    "* y\n",
    "* learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=X.shape, dtype=tf.float32) # input node for X\n",
    "outputs = tf.placeholder(shape=y.shape, dtype=tf.float32) # input node for y\n",
    "lr = tf.placeholder(shape=[1], dtype=tf.float32) # input node for learning rate\n",
    "\n",
    "## define loss graph.\n",
    "# after calling this function we have a global graph that computes loss\n",
    "# we can execute this global graph multiple times to get the loss depending on inputs and outputs\n",
    "l = loss(inputs, w, b, outputs) \n",
    "\n",
    "## get gradients of loss function w.r.t w and b\n",
    "dw, db = tf.gradients(l, [w, b])\n",
    "\n",
    "## define update nodes for w and b\n",
    "update_w = w.assign(w - lr * dw)\n",
    "update_b = b.assign(b - lr * db)\n",
    "\n",
    "## now define initialization node\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# define session and initialize variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed values into the placeholders use `feed_dict` keyword argument of the `sess.run(...)` function.\n",
    "\n",
    "`feed_dict` takes a dictionary with placeholders as keys and their corresponding inputs as values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200, loss: 1.4100929498672485\n",
      "Epoch: 20/200, loss: 1.3785699605941772\n",
      "Epoch: 40/200, loss: 1.3495323657989502\n",
      "Epoch: 60/200, loss: 1.322784662246704\n",
      "Epoch: 80/200, loss: 1.2981462478637695\n",
      "Epoch: 100/200, loss: 1.275450348854065\n",
      "Epoch: 120/200, loss: 1.254543423652649\n",
      "Epoch: 140/200, loss: 1.2352851629257202\n",
      "Epoch: 160/200, loss: 1.2175456285476685\n",
      "Epoch: 180/200, loss: 1.2012044191360474\n",
      "Final loss: 1.187\n",
      "W = [1.6865658], B = [2.750188]\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "learning_rate = np.array([0.001])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sess.run([update_w, update_b], feed_dict={\n",
    "        lr: learning_rate,\n",
    "        inputs: X,\n",
    "        outputs: y\n",
    "    })\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch: {}/{}, loss: {}'.format(\n",
    "            epoch, epochs, sess.run(l, feed_dict={\n",
    "                lr: learning_rate,\n",
    "                inputs: X,\n",
    "                outputs: y\n",
    "            })))\n",
    "\n",
    "w_final, b_final = sess.run([w, b])\n",
    "print(\"Final loss: {:.3f}\".format(sess.run(l, feed_dict={\n",
    "    inputs: X,\n",
    "    outputs: y\n",
    "})))\n",
    "print(\"W = {}, B = {}\".format(w_final, b_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
